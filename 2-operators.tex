\chapter{What is $+$?}

One day  $+$ means to add natural numbers, the next day 
polynomials, later matrices.  
You can even add colors ``Yellow=Blue+Green''. When you program 
you learn to add strings
\begin{center}
\begin{notebookin}
print "Algebra " + " is " + " computation"
\end{notebookin}
\begin{notebookout}
Algebra is computation
\end{notebookout}
\end{center}
The $+$ is in fact a variable stand in for what we call \emph{binary operator}
or \emph{bi-valent operator}, as it takes in a pair, 
according to the grammar $\Box+\Box$.  The valence and the grammar 
of an operator comprise its  \emph{signature}.  

Unlike these notes, addition should only be used when it is grammatically 
correct e.g.\ $2+3$ rather than $+23$.  Think of this like any other language 
where there could be a dialect that evolves the operator's grammar and lexicon.  A program 
to add two lists could get away with the following linguistic drift:
\begin{center}
\begin{notebookin}
cat [3,1,4] [1,5,9]
[3,1,4] + [1,5,9]
\end{notebookin}
\begin{notebookout}[2]
[3,1,4,1,5,9]
[4,6,13]
\end{notebookout}
\end{center}
Obviously using \texttt{cat} avoided confusion with the later $+$ 
concept and was the better choice.
Challenge yourself to see both as addition and you will 
find addition everywhere. 

Since we are evolving we my as well permit multiplication as a binary operator 
symbol, changing the signature to $\Box \cdot \Box$, i.e. $2\cdot 4$; or
$\Box\Box$, e.g. $xy$.   Avoid $\Box\times \Box$,
we need that symbol elsewhere. Addition is held to high standards in algebra
(that it will evolve into linear algebra).  So when you are considering a
binary operation with few if any good properties, use a
multiplication inspired notation instead.   


Valence 1 operators include the negative sign $-\Box$ to create 
$-2$.  Programming languages add several others 
such as \lstinline{++i, --i} which are said to \emph{increment} 
or \emph{decrement} the counter i (change it by $\pm 1$).
Programs also exploit a ternary (valence 3) operator:
\begin{center}
    \lstinline[language=Sava]{if (...) then (...) else (...)}
\end{center}
You may find that with shorter syntax like 
\lstinline{n!=0 ? m/n : error}.
If your interested look into \emph{variadic} operators to see how 
far this idea goes.

\subsection{Operators that generate}
Actually incrementing is more a definition than it is a 
special case of adding 1.
A natural number is either $0$, or a successor 
$S(k)$ to 
another natural number $k$.  Often this is expressed as a 
definition where $\mid$ stands for separating cases, 
for example:
\begin{align*}
    \mathbb{N} \defeq 0 \mid S(k)
\end{align*}
So $0$ is ``zero'', and $1$ is just a symbol representing $S(k)$, 
$2=S(S(0))$ and so on.  Replace $S$'s with tally marks 
we recover childhood counting:
\begin{center}
    $0\defeq$ \underline{\hspace{5mm}}, 
    $1\defeq$ \StrokeOne,
    $2\defeq$ \StrokeTwo,
    $3\defeq$ \StrokeThree,
    $4\defeq$ \StrokeFour,
    $5\defeq$ \StrokeFive,...
\end{center}
The point is, the successors are not so much a function 
moving around the numbers we have, it actually is a producer 
of numbers. 

Perhaps because it is so primitive, this is an idea we 
can imitate to create more meaningful values, like a string 
of characters in an alphabet \lstinline{['a','b',...,'z']}.
\begin{lstlisting}[language=Hidris]
data String = Empty | Prepend( head, tail) 
\end{lstlisting}
Some readers might relate to a different dialect of 
programming such as the following
\begin{lstlisting}[language=Sava]
class String
    case Empty extends List
    case Prepend( head, tail) extends List
\end{lstlisting}
The head here caries around what we put in the list and the tail 
is what comes next in the list.  Observe the similarities:
\begin{align}
     2 & \defeq S(S(0)) \tag{$\mathbb{N}$}\\
 \text{\lstinline{"me"}} & \defeq \text{\lstinline{Prepend('m',Prepend('e',Empty))}}
\tag{String}
\end{align}
The left-hand sides are merely notation for what the data really is on the right.
Both the successor and the \lstinline{Prepend} are operators that generate 
new values.  So part of algebra is to generate new data; so, it is no wonder 
that is closely connections to computation.

\subsection{Generated operators}
We can take this the idea of generating further, for example, using 
the unary operator, successor, prepend, etc., and have them generate 
binary operators.
\begin{align*}
    m+n & \defeq\left\{ 
    \begin{array}{ll}
        n & m = 0\\
        S(n+k) & m=S(k)
    \end{array}
    \right.
\end{align*}
So does $2+4=6$?  We can test this out.
\begin{align*}
    \text{\StrokeTwo}+\text{\StrokeFour} & = \text{\StrokeOne}~ \big(\text{\StrokeOne} +\text{\StrokeFour}\big)\\
    & = \text{\StrokeOne}~ \big( \text{\StrokeOne}~\big(\underline{\hspace{5mm}}+\StrokeFour\big)\big)\\
    & = \text{\StrokeOne}~ \big( \text{\StrokeOne}~\StrokeFour\big)\\
    & = \text{\StrokeOne}~ \text{\StrokeFive}.
\end{align*}
Try this for strings
\begin{align*}
    s+t & \defeq\left\{ 
    \begin{array}{ll}
        s & t = \texttt{Empty}\\
        \texttt{Prepend}(x,n+tail) & t=\texttt{Prepend}(x,tail)
    \end{array}
    \right.
\end{align*}
What is \lstinline{"awe"+"some"}?  

While no one will seriously add integers as a tally, 
knowing that it can be done establishes a pattern which can be exported to other context
with meaningful new structure.  Just notice $3+4=4+3$ but not so with adding strings.
These siblings have their own personalities, and it this might even help us recognize 
that while $3+4$ does equal $4+3$ it might be for somewhat subtle reasons.

What about multiplication? Isn't is just this:
\begin{align*}
    m\cdot n\defeq \overbrace{n+\cdots +n}^m.
\end{align*}
That is nice, but seems to leave us to figure out missing parenthesis or set aside 
time to prove they don't matter.  I am in the mood to continue making things rather 
than study them.  Lets repeat what we have done.
\begin{align*}
    m\cdot n & \defeq \left\{
        \begin{array}{ll}
            0 & m=0\\
            k\cdot n+n & m=S(k).
        \end{array}
    \right.
\end{align*}
It works, but check it out for yourself.  And for strings what might we get?
\begin{align*}
    s\cdot t & \defeq \left\{
        \begin{array}{ll}
            0 & s=\texttt{Empty}\\
            \texttt{Prepend}(x,k\cdot n)+n & s=\texttt{Prepend}(x,tail)
        \end{array}
    \right.
\end{align*}
What is "Mua"$\cdot$"Ha"?
%% Mua*Ha=MuaHaHaHa
You can carry on to make exponents and more.  If you do you may find 
Knuth arrow notation helpful on your journey, look it up.

\subsection{Operators measuring defects}
Algebraist spend a lot of time worried about misbehaving operators 
causing them to generate new operators that spot the flaws.  If 
we can add and multiply then we can make the following operators as well.
\begin{align*}
    [a,b] & = ab-ba \tag{Commutator}\\
    (a,b,c) & = a(bc)-(ab)c \tag{Associator}
\end{align*}
Commutative algebra requires $[a,b]=0$ while associative algebra needs $(a,b,c)=0$.
For example matrices fail to be commutative algebra but are associative.
Replace the role of multiplication of matrices with $[a,b]$ and ask for it's 
associate, i.e.
\begin{align*}
    (a,b,c)_{[,]} & = [a,[b,c]]-[[a,b],c]
\end{align*}
and we no longer get associative nor commutative algebra.  This are structures 
known as Lie algebras.  While not associative, because they are based originally 
on matrix products that are associative we can stumble eventually upon a 
graceful alternative
\begin{align*}
    0 & = [a,a] \tag{Alternating}\\
    0 & = [a,[b,c]]+[b,[c,a]]+[c,[a,b]].    
    \tag{Jacobi}
\end{align*}
So the problem is not getting worse, at least we wont be needing 
to look into some  valence 4 operators as defects.  Sabinin algebra 
studies how defects in operators pile up or die off.



Keep in mind requiring that $[a,b]=0$ or $(a,b,c)=0$ is an equation. 
Like any equation it has limited solutions.  By that reasoning, 
most of algebra wont be behave nicely.  


