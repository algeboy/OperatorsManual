\section{Order of operations.}
There is something still missing.  Try reading $\neg(x_2\wedge y)\vee x_1$.  
Is this meant to negate $(x_2\wedge y)\vee x_1$ 
or is it to negate $(x_2\wedge y)$ and then or that with $x_1$? This grammar 
offers two associated parse trees.
\begin{center}
    \begin{tikzpicture}
        \node at (-3.5,0) {\begin{tikzpicture}
            \node (10) at (0,0) {$\neg(x_2\wedge y)\vee x_1:Bool$};
            \node (0n) at ( 0,-2) {$(x_2\wedge y)\vee x_1:Bool$};
            % \node (0d) at ( 2,-4) {0:Digit};
            \draw[thick] (0n) -- (10);
            % \node[below of=10,scale=0.75] {$\circ$};
            \node at (0.5,-1) {$\neg$};
            % \node[scale=0.75,text width=0.6in] at ( 1.5,-3) {Nat case 1};
        \end{tikzpicture}};
        \node at (3.5,0) {\begin{tikzpicture}
            \node (01) at (0,0) {$\neg(x_2\wedge y)\vee x_1:Bool$};
            \node (1) at (-2,-2) {$\neg(x_2\wedge y):Bool$};
            \node (0n) at ( 2,-2) {$x_1:Bool$};
            \draw[thick] (0n) -- (10) -- (1);
            \node[below of=01] {$\vee$};
        \end{tikzpicture}};
    \end{tikzpicture}
\end{center}

\emph{Order of operations} steps in to resolve the ambiguity.
One option is \emph{left-most outer-most (LeMOM)}, which---similar to English language,
reads from left to right and assumes we start to match the pattern from the 
outer most symbols.  So  $\neg(x_2\wedge y)\vee x_1$ in LeMOM means
$\neg$ is the symbol we need to parse first.  That requires us to 
build a \lstinline{<Bool>}.  So we move left and read 
$(x_2\wedge y)\vee x_1$.  Here the LeMOM is `(' which will need to match 
with \lstinline{<Bool>)}.  Reading further $x_2:Var$ which promotes to 
$x_2:Bool$, but this is not followed by `)' so it must continue left-ward.
Next is $\wedge$ which matches with $x_2:Bool\wedge y:Bool$
so that we have $x_2\wedge y:Bool$.  Then we hit `)' which matches 
our earlier `('.  Now we finally get $(x_2\vee y):Bool$ which finally matches with 
$\neg$\lstinline{<Bool>}.  We have parsed $\neg(x_2\wedge y):Bool$.  At this point 
we have parsed $\neg$ and so we continue with $\vee$ to match 
$\neg(x_2\wedge y)\vee x_1:Bool$ with the right-hand-side parse tree.


LeMOM parsing is favored in computer science because we can 
decide what things mean the moment we encounter them in the string.
Meanwhile high school PEMDAS can require us to scan back and forth 
to know what to do, for example $7\cdot (7+3)^2$ could start by taking 
apart the product but then had to scan to the end to identify the exponent,
then back-track to insert the addition.  
There are other orders of operations include right-most inner-most (RiMIM).
The point of different orders of operations is to give the write the ability 
to write expressive language, jargon which means that more meaning is packed 
into the same space of letters.



% \begin{definition}
%     The \emph{parse-graph} of an accepted string $\sigma$ is 
%     defined recursively as the having vertices any production rule 
%     which ends in a terminal equal ot $\sigma$, or 
%     if $\sigma$ is non-terminal, then vertex for each production rule 
%     that accepts $\sigma$ with an edge to any production rules traversed
%     by that acceptance.
% \end{definition}

\begin{proposition}
    The parse graph of an accepted string of an unambigiuous context free grammar 
    is a unique vertex-labeled tree, with each label a production rule and each leaf 
    an atom (constant/variable symbol).
\end{proposition}
\begin{proof}
    Suppose $x$ is accepted as $x:Token$.  
    Because the grammar is an unambigious grammar 
    it has a unique token that accepts $x$.
    As it is context-free we are guaranteed 
    that $x$ was accepted by a production rule of form 
    \code{<Token>::= A1 ... An} where the 
    \code{Ai}'s are 
    terminal or production rules.  By recursion assign 
    to each \code{Ai} its associated parse graph which
     by induction makes each into a rooted tree.
\end{proof}

A further property that we often exploit but we shall not prove here is that 
context-free grammars are economical to parse.  In fact they do not even 
require the full strength of a computer but could be done with the type 
of system needed to program a microwave.  While Moore's law drove the creation 
of full strength (Turing Complete) microprocessors, the need to reduce energy 
and cost of production has brought a resurgence of such limited ability 
hardware.  
\begin{proposition}
    A context free grammar can be parsed by an automata with a push-down (last-in-first-out)
    stack.
\end{proposition}

For comparison, to parse context-sensitive grammars we suddenly require more 
than Turing machine, we need a nondeterminisitc machine.  That is, we have to make 
some (possibly random) choices about what way to parse.

