
\section{Operators for abstraction}
So algebra need operators, and operators as far as we have seen take 
inputs and give outputs.  In other words they are functions.  
Yet a number of subtle problems are 
mounting.  For example, we probably want a Boolean (true/false) to 
go in the the left-most spot of an if-then-else-, and we cannot compose 
just any two functions and get expected results.  We scale a vector on one side 
by not the other.  Can functions help sort this out?



As far as simplicity goes the following two example functions win 
on the grounds of requiring nothing, not even a domain or codomain.
\begin{align*}
    I(x) & \defeq x & 
    K_c(x) & \defeq c.
\end{align*}
You may call $I$ the identity function and the $K$ constant functions,
a different one for each $c$.  You can put anything into these functions.
Try some substitutions, I tried $I(3)=3$, $I(\clubsuit)=\clubsuit$.
I found $K_3(2)=3$ and $K_3(\clubsuit)=3$ as well.  I even tried 
$K_{\clubsuit}(2)$ and got $\clubsuit$.  I changed $x$ for $y$, 
$I(y)=y$, and $d$ for $c$, $K_d(x)=d$. Next I substituted $x$ for $c$ and got
\begin{align*}
    K_x(x)=x=I(x).
\end{align*}
Now we have a true problem: a constant function should not equal 
an identity function.  This is the \emph{paradox of the trapped (or captured) variable}.


Paradoxes (para = distinct + dox = opinion) are places where logical reasoning
leads to two seemingly opposing conclusions.  We could just as well call this an
inconsistency and declare the topic dead, but usually mathematics reserves the
word paradox for settings where we could avoid the inconsistency by revisiting
some fundamental notion and choosing a narrower interpretation.  True
inconsistency we reserve for cases where our only option is to throw out the
system.  For example, we could attempt to withhold some options that thus
prevent one of the two opinions to surface.  It is not a philosophically
satisfying resolution, it is choice or even a belief. As with most religions there
will be schisms.  I will here follow the church, that is Alanzo Church.

\begin{quote}
\emph{A system studied long enough reveals its paradoxes.}\\
~\hfill-- Dustin Tucker
\end{quote}

\subsection{Substitution with bound variables}
To define a function we need inputs and outputs, and that means to specify that
some variables are the ones we want to replace.  Often we denote this by given a
function a name, e.g.\ $I(x)=x$ gave the function the name $I$ and the proceeded
to identify its variables. Likewise $K_c(x)=c$ is a function of $x$, but not of
$c$.  Finally the names are not important.  We are just as informed to write
$x\mapsto x$ or $x\mapsto c$.  Church used the notation $\lambda x.x$ and
$\lambda x.c$ because it fits in with other notation such as $\forall x.P$ and
$\exists x.Q$ and so on.  In programs these are known as \emph{anonymous
functions} or as $\lambda$'s.   The variable set apart as the input is said to
be \emph{bound} to the function, it now has a specific role to play similar to
how $\forall x.P$  and $\exists x.Q$ bind $x$ into a special role as well.
Variables that do not have special roles are called \emph{free}.  Note that the
special role of $x$ in $x\mapsto M$ is limited to $M$.  We can say $M$ is the
\emph{scope}.  For this reason programs typically refer to bound variables a
\emph{local}.

Now let us describe how we substitute into functions
\begin{description}
    \item[Bound.match] $(x\mapsto M)[x\leftrightarrows N]\defeq (x\mapsto N)$
    \item[Bound.trapped]
    if $x$ is a free variable in $M$ and $y$ is a free variable in $N$ then 
    $(y\mapsto M)[x\leftrightarrows N]\defeq (y'\mapsto (M[y\leftrightarrows y'])[x\leftrightarrows N])$ where 
    $y'$ is a variable distinct from any in $M$ and $N$ (which exists 
    as there are unbounded numbers of variables);

    \item[Bound.other] $(y\mapsto M)[x\leftrightarrows N]\defeq (y\mapsto M[x\leftrightarrows N])$.
\end{description}


\subsection{Reduction}
The purpose of binding a variable is to control substitution but it may seem 
with rules like $(x\mapsto M)[x\leftrightarrows N]\defeq (x\mapsto M)$ that 
we will never get to evaluate a function.  To the contrary we are simply forcing 
ourselves to explicitly declare evaluation as a step, a computation, which 
very well may require work and alter our data.  This is done with a \emph{reduction}
denoted by $\leadsto$ and reads as ``leads to''.\index{$\leadsto$}\index{leads to}
\begin{description}
    \item[$\alpha$-reduction] given variables $x$ and $y$, with $y$ not free in $N$,
    \[(x\mapsto M)\equiv (y\mapsto M[x\leftrightarrows y]),\]
    which simply renames the variable $x$.

    \item[$\beta$-reduction]
    $(x\mapsto M)N\leadsto M[x\leftrightarrows N]$, which replaces all (free)
    instances of $x$ in $M$ with $N$.  This is the main tool, it is evaluation.
    
    \item[$\eta$-reduction]
    $(x\mapsto M)\leadsto M$, which unbinds the variable $x$.
\end{description}

The $\beta$-reduction rule is what finally gives meaning to ``evaluate'' a function.

\begin{theorem}[Church-Rosser]
    If a formula reduces in finite number of steps then all ways to reduce it give the same 
    final answer.
\end{theorem}


\begin{definition}
    A function is a sentence in $\lambda$-calculus.
    Evaluating a function is to apply $\beta$-reductions.
\end{definition}

\begin{corollary}
    If a function evaluates in finite time then whatever process it uses 
    gives the same answer.
\end{corollary}

% \subsection{Combinators}
% At some point you will expect your function might be used and if it is done 
% by a microprocessor that will mean translate all these substitution into a 
% sequence of basic operations, called \emph{gates} or simply \emph{combinators}.
% Of course the substitution rules are few so we can encode each by a small 
% number of combinators.  Thee in fact suffice, two which we have already meet, 
% $I$ and $K$.  We add to this the final pure combinator 
% \begin{align*}
%     S_{fg}(x) & \defeq (f(x),g(f(x))).
% \end{align*}
% With combinations of $S$, $K$, and $I$ we can enact all operations of substitution
% and therefore all of $\lambda$-calculus is a sequence of $SKI$'s, the ``Ski'' combinators.
% In fact combinators do not even require variables to be defined, we just give their 
% $\beta$-reductions and that is all we need.  Here they are.
% \begin{align*}
%     IM & \leadsto M\\
%     KMN & \leadsto M\\
%     SLMN & \leadsto LM(LN)
% \end{align*}
% In practice writing programs purely from SKI is tedious and inefficient.  We can 
% instead rely on circuits that are compute equivalent values and take advantage 
% of realistic bounds on the data.  For this reason, microprocessors and 
% quantum computers alike add many more combinators to achieve useful tasks 
% such as adding multiplying, and, or and more.  These are called \emph{applied combinators}.
% With these concepts in mind we can lay out the layers we find.

% \begin{itemize}
%     \item Turing machine: universal computer in that it can be reprogrammed to 
%     mimic all others.

%     \item Combinators: a universal assembly language, in that it reduces all 
%     programs into a sequence of primitive steps from a finite list of primitives.

%     \item $\lambda$-calculus: a universal programming language, in that it can 
%     encode any computable function.

% \end{itemize}

\subsection{Types of functions}

Finally it helps at times to have constraint on inputs
so that we can predict outcomes.  Most algebraic operators 
have this constraint. The way we manage this is to agree 
to use data in limited ways.  For example, if we want $n$ 
to be a natural number then we are interested in how many 
steps it is away form $0$ and not be bothered with cosmetic 
choices like how we store it.  For example,  $19$ is a digital
number, $19.0$ is in decimal form, and \code{0x13} is the same 
in Hexidecomal.  Yet if we write $19\in\mathbb{N}$ 
$19.0\in \mathbb{N}$, or even \code{0x13:Nat} we are announcing 
to all that we only care that 19 is a natural number, it is 
19 steps away form $0$ and nothing else about it will matter 
in what we do next.  This is abstraction in pure form: limit 
reasoning to agreed to properties.

By letting go of nuance to see data for limited qualities 
we are freed up to make predictions about simpler properties.
If I have a natural number $n$ and $f(n)=1/(n+1)$ then 
I know the output is a rational number.  I have the following 
conclusion.
\begin{gather*}
    \begin{array}{rl}
        f&:\mathbb{N}\to \mathbb{Q}\\
        n&:\mathbb{N}\\
    \hline 
        f(n) &:\mathbb{Q}
    \end{array}
\end{gather*} 
Does this look like reasoning elsewhere?  Perhaps it reminds 
you of the following.
\begin{gather}
    \tag{Modus Ponens}
    \begin{array}{rl}
        P &\Rightarrow Q\\
        P &\\
    \hline 
        Q
    \end{array}
\end{gather} 
One of the best ways to reason about data is to imitate 
how we reason about claims.  If we have data $a$ that we limit 
to agreed upon rules $A$ of abstraction, written $a:A$, 
then things we can state about propositions $P$ will be translated 
into claims about manipulating data $a:A$.  We call 
$A$ the \emph{type} of $a$.  So here is 
what the function type.

First we need to form the type, which depends on identifying 
the domain and codomain types.
\begin{gather}
    \tag{$\form{\to}$}
    \frac{A,B:Type}{A\to B:Type}
\end{gather}
This rule formed a new type, it is an operator: 
$(A,B)\mapsto (A\to B)$.  We call such rules \emph{formation}
or \emph{type builders}.

How  do we make data of a type?  We don't!  Data already exists 
all we want to do is draw attention to an abstraction, a use 
of the data limited by rules.  So we already have functions,
Church's $\lambda$-calculus, Schonefinkel-Curry Combinators,
or G\"odel's recursive functions.  We have picked up 
$\lambda$-calculus so let us add types to these.
\begin{gather}
    \tag{$\intro{\to}$}
    \frac{
        \Gamma, a:A \vdash M[x\leftrightarrows a]:B
    }{
        \text{func}(x\mapsto M):A\to B
    }
\end{gather}
This says if we have a string $M$ and a variable $x$, 
and we have a proof that replacing $x$ with $a:A$ then 
$M[x\leftrightarrows a]:B$ then we can think of 
$M$ as providing us with a function $A\to B$.
The term `func' is just a label, a \emph{tag} the 
marks the data as having been typed.  We also 
pause to bind $x$ to $M$ so that later we will know exactly 
what variable in $M$ to replace.  The $x$ is part of the data.
Rules that introduce a type to data are called \emph{introductions}.

We already know how we plan to use typed functions:
take inputs of a type and output the predictable type.
Rules like this area called \emph{elimination} because 
they eliminate the type we had to produce something new.
\begin{gather}
    \tag{$\elim{\to}$}
    \begin{array}{rl}
        f&:A\to B\\
        a&:A\\
    \hline
        f(a)&:B
    \end{array}
\end{gather}
Finally the introduction and elimination rules do not yet 
relate to each other.  For example we wrote $f:A\to B$ in the 
elimination rule not func($x\mapsto M$).  That omission is 
purposeful.  It is simpler to expose what elimination does 
by giving the notation we all expect for functions, the $f(x)$
notation.  But now we need to tell each other what this all 
means.  This needs a \emph{computation rule}
\begin{gather}
    \tag{$\comp{\to}$}
    \begin{array}{rl}
        a:A&\vdash M[x\leftrightarrows a]:B\\
        \acute{a}&:A\\
    \hline
        f(a)&\defeq (x\mapsto M)\acute{a}
    \end{array}
\end{gather}
Recall form the $\beta$-reduction rule 
$(x\mapsto M)\acute{a}\leadsto M[x\leftrightarrows \acute{a}]$,
but the reason not to write this directly is to give 
our computation some options.  For example, 
often it is acceptable to delay evaluation, write $\sqrt{2}$
or $\sin 7$, rather than spend the effort figuring out what 
those values truly equal as decimals.  Leaving off the 
result of $\beta$-reduction gives us the option to do this 
later if ever.


\begin{theorem}
    $(A\times B\to C)\longleftrightarrow (A\to (B\to C))$.
\end{theorem}