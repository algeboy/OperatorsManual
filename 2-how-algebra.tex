\chapter{What is $+$?}

One day  $+$ means to add natural numbers, the next day 
polynomials, later matrices.  
You can even add colors ``Yellow=Blue+Green''. When you program 
you learn to add strings
\begin{center}
\begin{notebookin}
print "Algebra " + " is " + " computation"
\end{notebookin}
\begin{notebookout}
Algebra is computation
\end{notebookout}
\end{center}
The $+$ is in fact a variable stand in for what we call \emph{binary operator}
or \emph{bi-valent operator}, as it takes in a pair, 
according to the grammar $\Box+\Box$.  The valence and the grammar 
of an operator comprise its  \emph{signature}.  

Unlike these notes, addition should only be used when it is grammatically 
correct e.g.\ $2+3$ rather than $+23$.  Think of this like any other language 
where there could be a dialect that evolves the operator's grammar and lexicon.  A program 
to add two lists could get away with the following linguistic drift:
\begin{center}
\begin{notebookin}
cat [3,1,4] [1,5,9]
[3,1,4] + [1,5,9]
\end{notebookin}
\begin{notebookout}[2]
[3,1,4,1,5,9]
[4,6,13]
\end{notebookout}
\end{center}
Obviously using \texttt{cat} avoided confusion with the later $+$ 
concept and was the better choice.
Challenge yourself to see both as addition and you will 
find addition everywhere. 

Since we are evolving we my as well permit multiplication as a binary operator 
symbol, changing the signature to $\Box \cdot \Box$, i.e. $2\cdot 4$; or
$\Box\Box$, e.g. $xy$.   Avoid $\Box\times \Box$,
we need that symbol elsewhere. Addition is held to high standards in algebra
(that it will evolve into linear algebra).  So when you are considering a
binary operation with few if any good properties, use a
multiplication inspired notation instead.   


Valence 1 operators include the negative sign $-\Box$ to create 
$-2$.  Programming languages add several others 
such as \lstinline{++i, --i} which are said to \emph{increment} 
or \emph{decrement} the counter i (change it by $\pm 1$).
Programs also exploit a ternary (valence 3) operator:
\begin{center}
    \lstinline[language=Sava]{if (...) then (...) else (...)}
\end{center}
You may find that with shorter syntax like 
\lstinline{n!=0 ? m/n : error}.
If your interested look into \emph{variadic} operators to see how 
far this idea goes.

\subsection{Operators that generate}
Actually incrementing is more a definition than it is a 
special case of adding 1.
A natural number is either $0$, or a successor 
$S(k)$ to 
another natural number $k$.  Often this is expressed as a 
definition where $\mid$ stands for separating cases, 
for example:
\begin{align*}
    \mathbb{N} \defeq 0 \mid S(k)
\end{align*}
So $0$ is ``zero'', and $1$ is just a symbol representing $S(k)$, 
$2=S(S(0))$ and so on.  Replace $S$'s with tally marks 
we recover childhood counting:
\begin{center}
    $0\defeq$ \underline{\hspace{5mm}}, 
    $1\defeq$ \StrokeOne,
    $2\defeq$ \StrokeTwo,
    $3\defeq$ \StrokeThree,
    $4\defeq$ \StrokeFour,
    $5\defeq$ \StrokeFive,...
\end{center}
The point is, the successors are not so much a function 
moving around the numbers we have, it actually is a producer 
of numbers. 

Perhaps because it is so primitive, this is an idea we 
can imitate to create more meaningful values, like a string 
of characters in an alphabet \lstinline{['a','b',...,'z']}.
\begin{lstlisting}[language=Hidris]
data String = Empty | Prepend( head, tail) 
\end{lstlisting}
Some readers might relate to a different dialect of 
programming such as the following
\begin{lstlisting}[language=Sava]
class String
    case Empty extends List
    case Prepend( head, tail) extends List
\end{lstlisting}
The head here caries around what we put in the list and the tail 
is what comes next in the list.  Observe the similarities:
\begin{align}
     2 & \defeq S(S(0)) \tag{$\mathbb{N}$}\\
 \text{\lstinline{"me"}} & \defeq \text{\lstinline{Prepend('m',Prepend('e',Empty))}}
\tag{String}
\end{align}
The left-hand sides are merely notation for what the data really is on the right.
Both the successor and the \lstinline{Prepend} are operators that generate 
new values.  So part of algebra is to generate new data; so, it is no wonder 
that is closely connections to computation.

\subsection{Generated operators}
We can take this the idea of generating further, for example, using 
the unary operator, successor, prepend, etc., and have them generate 
binary operators.
\begin{align*}
    m+n & \defeq\left\{ 
    \begin{array}{ll}
        n & m = 0\\
        S(n+k) & m=S(k)
    \end{array}
    \right.
\end{align*}
So does $2+4=6$?  We can test this out.
\begin{align*}
    \text{\StrokeTwo}+\text{\StrokeFour} & = \text{\StrokeOne}~ \big(\text{\StrokeOne} +\text{\StrokeFour}\big)\\
    & = \text{\StrokeOne}~ \big( \text{\StrokeOne}~\big(\underline{\hspace{5mm}}+\StrokeFour\big)\big)\\
    & = \text{\StrokeOne}~ \big( \text{\StrokeOne}~\StrokeFour\big)\\
    & = \text{\StrokeOne}~ \text{\StrokeFive}.
\end{align*}
Try this for strings
\begin{align*}
    s+t & \defeq\left\{ 
    \begin{array}{ll}
        s & t = \texttt{Empty}\\
        \texttt{Prepend}(x,n+tail) & t=\texttt{Prepend}(x,tail)
    \end{array}
    \right.
\end{align*}
What is \lstinline{"awe"+"some"}?  

While no one will seriously add integers as a tally, 
knowing that it can be done establishes a pattern which can be exported to other context
with meaningful new structure.  Just notice $3+4=4+3$ but not so with adding strings.
These siblings have their own personalities, and it this might even help us recognize 
that while $3+4$ does equal $4+3$ it might be for somewhat subtle reasons.

What about multiplication? Isn't is just this:
\begin{align*}
    m\cdot n\defeq \overbrace{n+\cdots +n}^m.
\end{align*}
That is nice, but seems to leave us to figure out missing parenthesis or set aside 
time to prove they don't matter.  I am in the mood to continue making things rather 
than study them.  Lets repeat what we have done.
\begin{align*}
    m\cdot n & \defeq \left\{
        \begin{array}{ll}
            0 & m=0\\
            k\cdot n+n & m=S(k).
        \end{array}
    \right.
\end{align*}
It works, but check it out for yourself.  And for strings what might we get?
\begin{align*}
    s\cdot t & \defeq \left\{
        \begin{array}{ll}
            0 & s=\texttt{Empty}\\
            \texttt{Prepend}(x,k\cdot n)+n & s=\texttt{Prepend}(x,tail)
        \end{array}
    \right.
\end{align*}
What is "Mua"$\cdot$"Ha"?
%% Mua*Ha=MuaHaHaHa
You can carry on to make exponents and more.  If you do you may find 
Knuth arrow notation helpful on your journey, look it up.

\subsection{Operators measuring defects}
Algebraist spend a lot of time worried about misbehaving operators 
causing them to generate new operators that spot the flaws.  If 
we can add and multiply then we can make the following operators as well.
\begin{align*}
    [a,b] & = ab-ba \tag{Commutator}\\
    (a,b,c) & = a(bc)-(ab)c \tag{Associator}
\end{align*}
Commutative algebra requires $[a,b]=0$ while associative algebra needs $(a,b,c)=0$.
For example matrices fail to be commutative algebra but are associative.
Replace the role of multiplication of matrices with $[a,b]$ and ask for it's 
associate, i.e.
\begin{align*}
    (a,b,c)_{[,]} & = [a,[b,c]]-[[a,b],c]
\end{align*}
and we no longer get associative nor commutative algebra.  This are structures 
known as Lie algebras.  While not associative, because they are based originally 
on matrix products that are associative we can stumble eventually upon a 
graceful alternative
\begin{align*}
    0 & = [a,a] \tag{Alternating}\\
    0 & = [a,[b,c]]+[b,[c,a]]+[c,[a,b]].    
    \tag{Jacobi}
\end{align*}
So the problem is not getting worse, at least we wont be needing 
to look into some  valence 4 operators as defects.  Sabinin algebra 
studies how defects in operators pile up or die off.



Keep in mind requiring that $[a,b]=0$ or $(a,b,c)=0$ is an equation. 
Like any equation it has limited solutions.  By that reasoning, 
most of algebra wont be behave nicely.  

\subsection{Operators forced into service}
Sometimes you have to be clever, even lucky, to guess an operator.
In famous history, Heisenberg escaped to an island from Copenhagen 
to avoid hay-fever, or an over zealous host Niels Bohr. 
Able to think clearly he summarized what he knew: a particle $\psi$
could take possibly many states, maybe spin up $\uparrow$ or spin down 
$\downarrow$, so it record this as linear combination
it as a linear combination $|\psi\rangle = \alpha|\uparrow\rangle+\beta|\downarrow\rangle$
where in vector space with $\{\uparrow,\downarrow\}$ as a basis. 

somewhat famously 
was having difficulty with hay-fever in Copenhagen and was stuck with 
quantum puzzles who could not solve. Clearing his head on an island 
he conjured the view that particles $\psi$ could be viewed as linear 
combinations of all the states the could o
$\langle \psi|=\alpha \langle $ consisting of the probabilities of each state.
recognized matrices capture all the states of a quantum system and 
to compute the next event involv


\subsection{Properties}
You can add music, well at least you can play two songs at the same time.
Is that again music?  Probably it is safer to say we can add sound, and 
some sound is music.  This is thinking like an algebraist, this is 
clarifying that addition has a context, sound in this case,
and the operations can be carried out without leaving that context.
We say that:
\begin{quote}
    ``Sound is closed under addition.''
\end{quote}
You needn't be too strict about the context.
When you add a list of length 3 to a 
list of length 3 it gets to a list of length 6, not 3.
Zoom out the context of lists of a general length and 
you regain closure.  This may not be enough.  When we divide 
we avoid division by $0$.  When we subtract natural numbers $m-n$
we need $m\geq n$.  When we compose functions we need the 
domain of the second to contain the image of the first.
You might be in a place to fix this, for example add $\pm\infty$ 
to define as $x/0$ or negative numbers to account for $3-5$.
In composable functions $f$ and $g$ can compose as relations:
\begin{align*}
    (g\circledcirc f)(x) \defeq \{z\mid \exists y, y=f(x), z=g(y)\}.
\end{align*}
When doing algebra with such operators we nearly always spend 
our time explore independent cases, most of which are 
roughly speaking ``errors''.  The better idea is to acknowledge 
we don't really want to divide by 0, produce negatives when we 
are studying positive numbers, and we shouldn't compose 
non-composable functions.


\subsection{Further operators}




These examples lean on some addition pre-existing somewhere.  





\section*{Other operators}



To enforce a philosophy such as that some algebra is closed to 
operators 
Such a philosophy is one 


To get the details rolling in earnest we must know the application.
The world around us partly interacts with computers so lets assume 
this is a uniform assumption: any problem in algebra that I want to 
state should be expressible to a computer.  That computer might not 
be able to handle it but its hard to imagine an equation we need 
to consider where the equation itself cannot be stored in a computer.




That is one philosophy, a philosophy where addition is an abstraction.
That word makes some bristle.  It reminds them of polarizing art 
installations or refrains from the bar room rants between pure and 
applied thinkers.  Abstract, for for those who reason, means to limit 
argument to specified attributes.  Thats every type of math and a good 
chunk of science so it shouldn't raise dismay.  \emph{Raising the level of 
abstraction} is then just the declaration the we're about to gather the 
instance we have and ma

Perhaps you feel some one owes you a foundation for addition, 
a place you derive what addition really means before you 
take to making it show up everywhere else.
You can count on it, literally.
\begin{align*}
    \mathbb{N} & = 0 | S~k.
\end{align*}


\chapter{How do we do algebra?}

This leaves us with the job of making those data which can be substituted 
into equations.  This means firstly new numbers that, like the decimals, complex 
numbers, polynomials or matrices; can give a meaning to things like $0$ and $1$
and eventually the answers $x$ we seek in solving applications.  Secondly 
we need to find what works for the operations, the sums, squares, products, and 
etcetera.  Last and most important in the method of algebra is to invent the possible 
substitutes for equality, what algebraist call \emph{congruences}.  

Over 2 centuries the methods have evolved and with it notation and emphasis to
the point where the three roles just articulated may not be recognizable.  
This is where it becomes necessary to add in constraints: a family of problems 
you need solved that guide you to the algebra you need.  But when we pause to see 
the similarities we can carry forward a far great number of algebraic lessons 
than we can by concentrating only on the best tools for individual examples.
That will be our perspective.  To further constrain the study we invite in 
applications that constrain the questions to important families the ones
which might solve your problem or lead you to the algebra the may one day define you.

