\section{Natural numbers as grammar}
Counting everyone knows is the start of mathematics, but I think it's lesson 
has more to do with grammar than anything else. 

\subsection{Introducing counting}
Early history (early childhood) is counting.  Count 
pebbles or beads and give the patterns names
\begin{center}
    $0\defeq$ \underline{\hspace{5mm}}, 
    $1\defeq$ \StrokeOne,
    $2\defeq$ \StrokeTwo,
    $3\defeq$ \StrokeThree,
    $4\defeq$ \StrokeFour,
    $5\defeq$ \StrokeFive,...
\end{center}
``0'' might have been chosen as the shape left by removing the last pebble from
a sand table leaving behind no pebbles.

It struck Giuseppe Peano in the early 1900's that tallies 
would be easier to get right mathematically than digits. With the following two rules
Peano introduced the natural numbers to the formalism of math.
\begin{quote}
    \textit{
    $N_0$ vale ``numero'', et es nomen commune de 0,1,2, etc.\\
    $0$ $\to$  ``zero''\\
    $+$ $\to$ ``plus''.  Si $a$ es numero, $a+$ indica ``numero sequente $a$''.
    }
\end{quote}
% It is fitting that the Italian is in italics.
(See G. Peano \emph{Formulaire de mathematiques.~I-V}, p.27.)
Replacing $+$ with \StrokeOne ~(and equating \StrokeFive$\defeq$\StrokeFour~\StrokeOne),
Peano's model of numbers is simply the grammar of tallies.

Today notation has evolved.  These numbers are now almost always designated as
\emph{natural numbers} and denoted $\mathbb{N}$.  Instead of $a+$ we now 
often write $S~k$  or $S(k)$ calling it the ``successor'' to the natural number $k$.  
Programming however has held closer to the original with notation like 
\code{i++} and \code{++i}.

We mentioned Peano was merely recording a grammar.  Today we write grammars with 
list of rules, called \emph{production rules}.  Some rules are to specify 
what makes up the alphabet of symbols in our grammar.  For Peano, $0$ and $S$
are the complete alphabet.  So we may write either $0$ on its own, or 
we may pre-pend the symbol $S$ to any existing natural number.  
Each rule is given a name called a \emph{token} (or \emph{tag}) and 
denoted \code{<Name>}. Since the Walrus
$\defeq$ is our assignment of variables (more on this later), 
we use the ``astonished Walrus'' $::=$
as assignment of production rules.   Taken together the grammar is this.
\begin{center}
\begin{gcode}[]
<Nat> ::= 0 
<Nat> ::= S <Nat>
\end{gcode}
\end{center}
In the first rule we are told \code{0} is a natural number, denoted
\code{0:Nat}. We say that the grammar \emph{accepts} \code{0} because it matched
some production rule.  In the second rule if we encounter an \code{S} it must be
followed by an \emph{already known} natural number.  So \code{S0:Nat} but
\code{0S} would not be accepted as it is not found as a production rule, we say
the grammar \emph{rejects} \code{0S}.

Graphically we can render accepted words with parse trees, in this case with no branches.
\begin{center}
    \begin{tikzpicture}
        \node (0) at (0,0) {0};
        \node (1) at (3,0) {S0};
        \node (2) at (6,0) {SS0};
        \draw[thick,->] (0) edge["S"] (1);
        \draw[thick,->] (1) edge["S"] (2);
    \end{tikzpicture}
\end{center}

\begin{definition}
A production rule that appears more than once is called \emph{inductive}.
An accepted short hand for inductive productions rules is to name it once 
and separate the cases by $\mid$, for instance,
\code{<Nat>::= 0 | S <Nat>} or 
\begin{center}
\begin{gcode}[]
<Nat> ::= 0 
        | S <Nat>
\end{gcode}
\end{center}
\end{definition}



\begin{remark}
    The power of the ``already known'' clause of the rules is to prevent ambiguity 
    with terms like $n=$\code{SSS....} where the \code{S} continue forever.
    See if we remove on \code{S} form $n$, the string is the same as $n$.
    Since we are engaged in deciding if $n$ is a natural number and its substring 
    is $n$, it is not of the form \code{S k} for \code{k:Nat}.  Hence the grammar 
    rejects such an $n$.  Grammar's like these are called \emph{primitive recursive}
    meaning that the recursion can only depend backwards 
    in history.
\end{remark}

We do well to acknowledge inductive types a basic programs.
Computers understand this much.
Listing~\ref{lst:peano} shows two programs you could run today that implement Peano's idea.
There are of course many differences.  Visibly, the left-hand side 
favors mathematically minded symbolic notation and 
economizes even on parentheses in the spirit of ``$\sin x$'' notation.
Meanwhile the right-hand side favors a verbose imitation of 
natural language and prefers the $\sin(x)$ notation.  Set the differences 
aside.

\begin{lstfloat}
\begin{center}
\begin{minipage}{0.4\textwidth}
\begin{Fcode}[]
data Nat = Z 
         | S k

zero = Z
two = S (S zero)
\end{Fcode}
\end{minipage}
\hfill
\begin{minipage}{0.59\textwidth}
\begin{Pcode}[language=Sava]
class Nat
    case Zero() extends Nat
    case Next(k:Nat) extends Nat
sealed  // no more cases
zero = new Zero()
two = new Next(new Next(zero))
\end{Pcode}
\end{minipage}
\end{center}
\caption{Peano's natural numbers programmed in two different languages.}
\label{lst:peano}
\end{lstfloat}
    

Even without a deep understanding of these programs, one can make out the
contours of Paeno's definitions.  Both use a mix of keywords (in blue) to tell
our system to prepare a new type (or class) of data that will be called
\code{Nat}.  Then they instruct the system to accept exactly two ways to make
such data. It may be some initial state, \code{Z}, respectively \code{Zero},
that depends on nothing; otherwise, we must give data \code{k} of type
\code{Nat}, denoted \code{k:Nat}, which will then produce new data \code{S k},
respectively \code{Next(k)}.  In many systems the keyword \code{new} is 
used to help clue the reader into the fact that this is some data being now 
created.  It helps remind us that numbers do not some exists on their own, 
we have to expend resources (energy \& storage) to create them. 

We can summarize this accomplishment in two logical rules, but first allow 
me to digress to explain three crucial differences in the logic of going from 
one list of facts to another.

\subsection{Three meanings of ``forward proof''}
 A sentence that may contain variables which when 
assigned can have a truth value is called a \emph{predicate}.\footnote{Historically it 
is also known as a \emph{proposition} but that word has so often been used exclusively 
for true statements that it now offers more confusion than then the completely uninformative word ``predicate''.}  
For example 
\begin{center}
    $P(n):\equiv$ ``$n$ is even''.
\end{center}
Witness $P(2)=\top$ (true), that is ``$2$ is even''.  
Notice $P(5)=\bot$ (is false).  In all cases these are truth values
so $P(n)$ is a predicate.  Notice truth values may be unknowable.
At present for example, taking 
\[ 
    f(n) =\begin{cases} n/2 & 2\mid n\\ 3n+1 & 2\not\mid n \end{cases}
\]
we can ask $P:\equiv \forall n.\liminf_{i\to \infty}f^i(n)=1$.
This could be true or false, but no one yet knows.  It is the Collatz conjecture.
Some wonder if it belongs to a class of statements that can never be known
as true or false.  If that is the case it would come down to an axiom 
(the Law of Excluded Middle if you use it) to assign it one of those choice.
However, logic is just as capable of using ``unknowable'' as a truth value.

Suppose that $P$ is a list of predicates 
and $q$ is a single predicate.  Then there are at least three possible 
ways that we might think of going from $P$ to $q$.
\begin{description}
    \item[Derives] write $P\vdash q$ when there is a proof from $P$ to $q$.
    (Formally: a proof is a rooted tree with root $q$, leaves either from $P$ or axioms, 
    and all internal nodes labeled by a logical operation.  In computer science 
    these are known as \emph{fan-in circuits}.)
    
    \item[Entails] write $P\vDash q$ when for every assignment of variables that makes 
    $P$ true, that assignment of variables makes $q$ true.

    \item[Implies] $p\Rightarrow q$ is merely a binary operator that takes
    predicates $p$ and $q$ and outputs a new predicate denoted $p\Rightarrow q$.
    So $P\Rightarrow q$ can take on a value of $\top$ but we cannot say 
    that it is true, it is just a predicate that it may be true.
\end{description}
A mathematician is prone to confuse these three and with good reason.
The first leads to the second, and the second leads to the third \emph{being true},
in symbols
\begin{itemize}
    \item $(p\vdash q)\vdash (P\vdash q)$.
    \item $\displaystyle (p\vDash q)\vdash \left(\vdash \left(\bigwedge_{p\in P} p\right) \Rightarrow q\right)$.
\end{itemize}
Notice $\vdash X$ means it takes nothing to prove $X$, in other words,
this is a fancy way to infer that $X$ is true.  

\begin{remark}
    So is it strictly speaking meaningful to write something like the 
    following? 
    \begin{quote}
        \textbf{Theorem.} $n=2k\Rightarrow \gcd(k,n+2)=2$.
        \hfill{\color{BrickRed} (Meaningless)}
    \end{quote}
    On the one hand this is not a statement of fact any more than 
    to say ``$n$ is even.''  To write $p\Rightarrow q$ is to give 
    a predict, not to argue that it is true.  A proper statement 
    might be 
    \begin{quote}
        \textbf{Theorem.} $n=2k\vdash \gcd(k,n+2)=2$.
        \hfill{\color{BrickRed} (Formally Correct)}
    \end{quote}

    On the other hand, the label ``Theorem'' could be tasked with the 
    role of decorating the predicate with an air of validity, as if 
    to implicitly tack on the ending ``...is true'' in invisible red ink:
    \begin{quote}
        \textbf{Theorem.} ``$n=2k\Rightarrow \gcd(k,n+2)=2$'' {\color{BrickRed} is true.}
    \end{quote}
    Yet, proponents of such fix should ponder if they really want to say
    Theorem. ``$n=2k$ is even'' is true.

    \noindent\rule{\textwidth}{1pt}
    Perhaps a solution worth considering is that in mathematics we 
    despense with the notation we do not entirely understand.  We may write 
    \begin{quote}
        \textbf{Theorem.} If $P$  then $q$.
        \hfill{\color{BrickRed} (Acceptable)}
    \end{quote}
\end{remark}
Finally, be mindful that G\"odel proved that the converse is false.  There
examples where we can write $P\vDash q$ but not $P\vdash q$.   This is sometimes
misrepresented as saying ``there are theorems which are true but cannot be
proved.'' That however looses the subtlety behind his claim by using the
vagueness of the word ``theorem'' to simultaneously mean both $\vDash$ and
$\vdash$.  More accurate reading would be that it may be impossible to find
counter-examples where $P$ is true and $q$ is false. Notice I did not say
``there are no counter-examples'' just that there is no process to find a
counter-example.  As everyone knows failure find counter-examples is not itself
a proof of anything. This leads one simply to surmise either by decree of axioms 
that such a situation should now count as a new form of proof, or to accept that 
some situations like this exist.

\subsection{Eliminating counting}
    
Now that we have created numbers we shall want to use them.
An obvious use of numbers in counting is to make the process 
modular and parallel.  For example we can have two separate counts 
later combined by addition.
\begin{center}
    \begin{tabular}{c|ccccc}
    ``add'' & \StrokeOne & \StrokeTwo & \StrokeThree & \StrokeFour & \StrokeFive\\
    \hline 
    \StrokeOne & \StrokeTwo & \StrokeThree & \StrokeFour & \StrokeFive & \StrokeOne \StrokeFive\\
    \StrokeTwo & \StrokeThree & \StrokeFour & \StrokeFive & \StrokeOne \StrokeFive & \StrokeTwo \StrokeFive\\
    \StrokeThree & \StrokeFour & \StrokeFive & \StrokeOne \StrokeFive & \StrokeTwo \StrokeFive & \StrokeThree \StrokeFive \\
    \StrokeFour & \StrokeFive & \StrokeOne \StrokeFive & \StrokeTwo \StrokeFive & \StrokeThree \StrokeFive & \StrokeFour \StrokeFive\\
    \StrokeFive & \StrokeOne \StrokeFive & \StrokeTwo \StrokeFive & \StrokeThree \StrokeFive & \StrokeFour \StrokeFive & \StrokeFive \StrokeFive\\
    \end{tabular}
    \hspace{1cm}
    \begin{tabular}{|c|cccccc|}
        \hline 
        + & 0 & 1 & 2 & 3 & 4 & 5\\
        \hline 
        0 & 0 & 1 & 2 & 3 & 4 & 5 \\
        1 & 1 & 2 & 3 & 4 & 5 & 6\\
        2 & 2 & 3 & 4 & 5 & 6 & 7\\
        3 & 3 & 4 & 5 & 6 & 7 & 8\\
        4 & 4 & 5 & 6 & 7 & 8 & 9\\
        5 & 5 & 6 & 7 & 8 & 9 & 10\\
    \hline
    \end{tabular}
\end{center}
Beyond a table we need some formulas.  If we have two groups $m$ and $n$ and each is either nothing 
or a successor to something else, then that leaves us with just four cases 
to consider.
\begin{align*}
    \begin{array}{|c|cc|}
        \hline 
        + & 0 & S(k)\\
        \hline 
        0 & 0 & S(k) \\
        S(\ell) & S(\ell) & S(S(\ell+k))\\
        \hline
    \end{array}
\end{align*}
As the first column does not change $m$ we can simplify this down to 2 cases.
\begin{align*}
    m+n \defeq \begin{cases} m & n=0\\ S(m+k) & n=S(k)\end{cases}
\end{align*}
Lets see this out one some examples, recalling that $1=S0$, $2=S1$, $3=S2$ and so on.
\begin{align*}
    3+2 & = S(3+1)= SS(3+0) = SS3=5.\\
    7+3 & = S(7+2) = SS(7+1) = SSS(7+0)=SSS7=10.
\end{align*}
Notice something about this process is eliminating, or consuming, 
our number $n$ in the sum $m+n$.  

And this process gives immediate significance to computation.
To use an inductive type in a program we simply run through the cases.
Programming languages allow for case-by-case analysis in numerous ways 
but most today offer some form of \emph{pattern matching} which is 
where we list the types of production rules and then attach the outcome 
for each.
\begin{lstfloat}
\begin{center}
\begin{minipage}{0.4\textwidth}
\begin{Fcode}[]
+:Nat->Nat->Nat
+ m  0    = m
+ m (S k) = + m k
\end{Fcode}
\end{minipage}
\hfill
\begin{minipage}{0.59\textwidth}
\begin{Pcode}[]
def add(m,n:Nat):Nat =
  match n with 
    Zero() => m
    Next(k)=> Next(add(m,k))
\end{Pcode}
\end{minipage}
\end{center}
\caption{Peano's addition of natural numbers programmed in two different languages.}
\label{lst:peano}
\end{lstfloat}


So in a sense our grammar's role is to guard that data given fits 
a pattern.  Once that has been accepted, when we encounter data 
of this grammar's type we can use that pattern to define functions 
that consume that data.  Historically this is known as \emph{eliminiation}
on account that it may eliminate the data given in the process of 
creating new data as output.

% This algorithm remains predictable because of what we assumed about 
% primitive recursion.  That is $m+0$ is defined and if $m+k$ is defined 
% then $m+S(k)\defeq S(m+k)$ is therefore defined.  This is what is 
% known as \emph{recursion}.  It hinges on the fact that the proper 
% \[    P(n) :\equiv m+n \text{ defined}
% \]
% we want is known at step $k$, we write $P(k)$, and so we can 
% use that knowledge to deduce the $m+S(k)$ is defined, in other wards $P(S(k))$ follows.
% \begin{gather*}
%     \begin{array}{rl}
%     P(0)& \\
%     P(k) & \Rightarrow P(k+1)\\
% \hline 
% \forall n:\mathbb{N} & P(n).
%     \end{array}
% \end{gather*}
% This is sometimes known as the principle of induction.  It is more of a program than 
% an assumption of fact.  When we have a $n$ we want to confirm $m+n$ is defined for,
% say $n=SS0$, then we simply apply the rules given repeatedly:
% \[
%     m+SS0=S(m+S0)= SS(m+0)=SS(m)
% \]
% By rule \code{S<Nat>} we get that $SS(m):\mathbb{N}$.  As expected it added two strokes to the 
% tally.   This process of consuming the information given to introduce $n:\mathbb{N}$ is known 
% as \emph{elimination}.   We will see several examples introduction 
% and elimination patterns. 

% If we stand back to observe the entire process, grammars allow us to introduce numbers, 
% such as $2\defeq SS0:\mathbb{N}$.  Then when we have $2:\mathbb{N}$ we can 
% use the data behind its introduction (here two successors and a $0$) as instructions 
% to follow when producing a new outcome, perhaps a new type of data.  This is known 
% as \emph{eliminating} the introduction.  We will see several examples introduction 
% and elimination patterns. 

% \subsection{Type rules}
% We are engaged in forming a type of data, natural numbers.  Some times 
% of data will depend on others, for example tuples $\mathbb{N}^k$ depend on 
% $k$.  So even in the form of types we need to pause and 


% If we break down the 
% steps we first had to give the type a name, 
% If we break this process down here are the summary points.  Note that we use Frege's 
% notation where symbols $p_1,\ldots,p_n$ that lead to symbols $q$ are separated by 
% $\vdash$, so $p_1,\ldots,p_n\vdash q$, which is also often written 
% \begin{align*}
%     \frac{p_1,\ldots,p_n}{q}\qquad 
%     \begin{array}{c}
%         p_1\\
%         \vdots \\
%         p_n\\
%     \hline 
%         q 
%     \end{array}
% \end{align*}
% Note that if nothing is required to lead to $q$ we can write $\vdash q$.
% The symbol $\vdash$ is known formally as \emph{entailment} and while it may 
% appear at first like an implication, it is here being used more in the role of 
% defining rules.
% \begin{gather}
%     \tag{$\form{\mathbb{N}}$}
%     \vdash \mathbb{N}:Type\\
%     \tag{$\intro{\mathbb{N}}$}
%     \vdash 0:\mathbb{N} 
%     \qquad \frac{k:\mathbb{N}}{S(k):\mathbb{N}}\\
%     \tag{$\elim{\mathbb{N}}$}
%     \begin{array}{rl}
%         n &:\mathbb{N}\\
%         P(k) &\vdash P(S(k))\\
%     \hline 
%         P(n)
%     \end{array}
% \end{gather}
