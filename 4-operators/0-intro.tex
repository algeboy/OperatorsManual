One day  $+$ means to add natural numbers, the next day 
polynomials, later matrices.  
You can even add colors ``Yellow=Blue+Green''. When you program 
you learn to add strings
\begin{center}
\begin{notebookin}
print "Algebra " + " is " + " computation"
\end{notebookin}
\begin{notebookout}
Algebra is computation
\end{notebookout}
\end{center}
If we focus on our 
speech we find more expansive uses:
``add the flour, water, yeast, and salt'' or  
``count each household, then add''.

These suggest that addition is a stand in, a variable.  It is not however 
an ordinary variable.  We don't replace $+$ sign with a number.  
Addition depends on some inputs, the total 
number being its \emph{valence}.    It also depends on a grammar.
It may be \emph{infix} $2+3$ as we commonly 
use with arithmetic, more specifically, if we intend to add just one type 
$A$ of data we want to add then its infix addition grammar is
\begin{center}
    \begin{minipage}{0.4\textwidth}
\begin{Gcode}[]
<A> ::= <A> + <A> 
\end{Gcode}
\end{minipage}
\end{center}
We abbreviate this to $\Box+\Box$ at times when confusion will be limited.
Addition may also be \emph{prefix} $+,2,3$ as in the recipe instructions,
or it could be \emph{postfix} $2,3,+$; as in the census illustration. 
Formally
\begin{center}
\begin{minipage}{0.4\textwidth}
\centering
\begin{Gcode}[]
<A> ::= + <A> <A> 
\end{Gcode}
$+ \Box \Box$
\end{minipage}
\hfill
\begin{minipage}{0.4\textwidth}
\centering
\begin{Gcode}[]
<A> ::= <A> <A> +
\end{Gcode}
$\Box \Box +$
\end{minipage}    
\end{center}    

The variations in grammar are like any language 
where there could be a dialect that evolves the operator's grammar and lexicon.
 HP calculators were postfix for some time to match engineering requirements.
A program to add two lists could get away with the following linguistic drift:
\begin{center}
\begin{notebookin}
cat [3,1,4] [1,5,9]
[3,1,4] + [1,5,9]
\end{notebookin}
\begin{notebookout}[2]
[3,1,4,1,5,9]
[4,6,13]
\end{notebookout}
\end{center}
Using \texttt{cat} reminded us to concatenate and avoided confusion with the
later $+$ concept.  It was the better choice. Challenge yourself to see both as
addition and you will find addition everywhere. 

Addition  may be with two or more terms.  It also may be 
sensitive to order: ``add the food coloring to the vinegar, then add the baking
powder'' is a different to ``add the vinegar and baking powder then the
food coloring''.  To make a manageable theory, algebra needs fewer variations that 
build towards this complexity.

\subsection{Variable Valence}
\index{variadic}
Add each of the following in your head as quickly as possible.
\begin{equation*}
\begin{array}{cc}
    & 13\\
    & 27\\
   + & 19\\
\hline\\
\end{array}
\hspace{1.5cm}
\begin{array}{cc}
    & 26\\
    & 18\\
  + & 42\\
\hline\\
\end{array}
\hspace{1.5cm}
\begin{array}{cc}
    & 91\\
    & 42\\
  + & 29\\
\hline\\
\end{array}
\hspace{1.5cm}
\begin{array}{cc}
    & 21\\
    & 21\\
  + & 21\\
\hline\\
\end{array}
\end{equation*}
If you are too proud for this then compute the following.
\begin{align*}
    \begin{bmatrix}
        1 & -1 
    \end{bmatrix}
    \begin{bmatrix}
        1 & 2 \\ 3 & 4 
    \end{bmatrix}
    \begin{bmatrix}
        -1 & 1 \\
        2 & 0
    \end{bmatrix},
    \qquad
    \begin{bmatrix}
        -1 & 1 \\
        2 & 0
    \end{bmatrix}
    \begin{bmatrix}
        1 & 2 \\ 3 & 4 
    \end{bmatrix}
    \begin{bmatrix}
        1 \\ 0 
    \end{bmatrix}
\end{align*}
Let me say that $x$ is the first row, 
$y$ the second, and $z$ the third.  So how did your calculations 
work?  I suspect the in the first you combined the numbers in the 
order you saw, so $(x+y)+z$ because combining 3 with 7 seemed easier.
In the second example I bet you shifted to $x+(y+z)$.  In the third 
you probably computed $(x+z)+y$.  The fourth I recon was not done by 
adding at all but by multiplying.  If you multiplied the matrices 
instead I nearly certain the one on the left you did left-first and 
the one on the right you did right-first.  

In truth, when we see triple sums or triple products we are operating 
on this data as if this is a genuinely triple question.  It is a dynamic process
by which we take in all the data and then decide how it is we will combine it
for the correct end result most efficiently.   Think this is a human crutch?
In programs we can add any finite list of $\sum_{i} n_i$ and 
programs back this up with commands like \code{sum(ns)} 
(the convention in programs is that a sequence $n_*$ is transcribed as 
the plural \code{ns}).  Likewise we can concatenate any 
number of strings.  Inside their code, these functions are given great freedom 
to divide the work up perhaps to take advantage to parallel computing 
or the location of the data.  Even in purely mathematical contexts,
today's algorithms for arithmetic on polynomials, matrices, and 
permutations introduce techniques to delay calculations until strictly 
necessary or computationally profitable. 

Let me demonstrate one common example. 
If I want to multiply two $100\times 100$ matrices $A$ and $B$ it 
will take me about $2,000,000$ additions. But If I know that 
the only way I plan to use $AB$ to apply it to a vector $v$,
then I can delay the multiplication of $A$ with $B$ and compute instead
\[
    A(Bv) = (AB)v.
\]
The right-hand side uses $40,000$ additions, just 2\% of what the 
first approach needed.  Nearly every Computer Algebra System (CAS) today 
employs some method of this kind.  
Witness this saving can only be effected once we have at least 3 matrices
to multiply.  It a genuine trivalent consideration.  Yet, no book
on linear algebra will define a triple matrix product as it seems 
multiplying just two matrices will be enough.  Obviously that is not the whole story.



The point is that operations in practice (both for proofs and calculations)
have variable numbers of inputs and are heterogeneous combinations.  
The purpose of chopping up operations into fixed valence is to have the means 
to study and describe what operations should equal when done properly. 
Defining 
\[ 
    x+y+z \defeq (x+y)+z
\] 
is not to say this is the right algorithm for adding triples, but simply to 
say that the correct answer should match this algorithm.



\index{bivalent!opeator}\index{binary operator|see{bivalent operator}}
Addition from here on out will be bivalent (also called binary), meaning 
that it requires 2 inputs.  We typically prefer infix grammar $\Box +\Box$.
Since we are evolving, we may as well permit multiplication as a bivalent operator
symbol, changing the signature to $\Box \cdot \Box$, i.e. $2\cdot 4$; or
$\Box\Box$, e.g. $xy$.   Avoid $\Box\times \Box$, we need that symbol elsewhere.
These days composition $\Box\circ\Box$ is written as multiplication; so, you can
use that symbol however you like.  Addition is held to high standards in algebra
(that it will evolve into linear algebra).  So when you are considering a binary
operation with few if any good properties, use a multiplication inspired
notation instead.   




\index{univalent!opeator}\index{unary operator|see{univalent operator}} Valence
1, also called \emph{univalent} or \emph{unary}, operators include the negative
sign $-\Box$ to create $-2$ as well as the transpose $A^{\dagger}$ of a matrix
$A$. Notice in the case of negative an integer remained an integer, but in the
case of transpose a $(2\times 3)$-matrix becomes a $(3\times 2)$-matrix.
Operators can change the type of data we explore.
 
Programming languages add several others univalent operators
such as \code{++i, --i} which are said to \emph{increment} 
or \emph{decrement} the counter i (change it by $\pm 1$).

Programs also exploit a trivalent (ternary) operator:
\begin{center}
\begin{Pcode}[]
if (...) then (...) else (...)
\end{Pcode}
\end{center}
The words, while helpful, are unimportant and some languages
replace it with symbols emphasizing it is an operator:
\begin{center}
    \pcode{_?_:_}
\end{center}
% Here for example is division with remainder of positive integers
% \begin{center}
% \begin{Pcode}[]
%     div(m,n)=(m>=n)?(div(m-n,n)+(1,0)):(0,m)
% \end{Pcode}
% \end{center}

